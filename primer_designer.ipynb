{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-g GFF3] -o OUTPUT\n",
      "                             [--forward_overhang FORWARD_OVERHANG]\n",
      "                             [--reverse_overhang REVERSE_OVERHANG]\n",
      "                             [--max_length MAX_LENGTH] [--truncate]\n",
      "                             fasta\n",
      "ipykernel_launcher.py: error: the following arguments are required: -o/--output\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The holy grail\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import List, Dict, Union\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from pydna.design import primer_design\n",
    "from pydna.dseqrecord import Dseqrecord\n",
    "\n",
    "\n",
    "\n",
    "def open_gff3_files(path: str = \"\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Opens and reads a GFF3 file and returns its contents as a list of lists.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path: str\n",
    "        The path to the GFF3 file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[List[str]]\n",
    "        A list of lists containing the contents of the GFF3 file.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as infile:\n",
    "        LINES = []\n",
    "        for line in infile:\n",
    "            LINES.append(line[:].split(\"\\t\"))\n",
    "        LINES = LINES[1:]\n",
    "\n",
    "    return LINES\n",
    "\n",
    "def read_gff_to_pd(path: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a GFF3 file and returns a pandas DataFrame with columns 'gene', 'start_pos', 'end_pos',\n",
    "    and 'signal_peptide_likelyhood'.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    path : str\n",
    "        The path to the GFF3 file.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas.DataFrame\n",
    "        A DataFrame with columns 'gene', 'start_pos', 'end_pos', and 'signal_peptide_likelyhood'.\n",
    "    \"\"\"\n",
    "\n",
    "    gff = open_gff3_files(path)\n",
    "    dict_of_signal_peptides = tidy_up_gff(gff)\n",
    "    df = pd.DataFrame.from_records(dict_of_signal_peptides)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_gff3_and_fasta(gff3_path: str, fasta_path: str):\n",
    "    \"\"\"\n",
    "    Reads the GFF3 and FASTA files and returns parsed data.\n",
    "    If no GFF3 file is given, returns only the sequences.\n",
    "    \"\"\"\n",
    "    sequences = list(SeqIO.parse(fasta_path, format='fasta'))\n",
    "    \n",
    "    if gff3_path:\n",
    "        signal_pep = read_gff_to_pd(gff3_path)\n",
    "        N_pos = signal_pep['end_pos'].to_list()\n",
    "        genes_with_signal_peptides = signal_pep['gene'].to_list()\n",
    "        return sequences, genes_with_signal_peptides, N_pos\n",
    "    \n",
    "    return sequences, None, None\n",
    "\n",
    "def tidy_up_gff(lst_of_gff: list) -> list:\n",
    "    \"\"\"\n",
    "    This function takes a list of GFF lines and returns a list of dictionaries,\n",
    "    with each dictionary containing information on the signal peptides in the GFF file.\n",
    "\n",
    "    Parameters:\n",
    "    lst_of_gff (list): A list of GFF lines.\n",
    "\n",
    "    Returns:\n",
    "    list_of_peptides (list): A list of dictionaries, with each dictionary containing information on the signal peptides in the GFF file.\n",
    "    \"\"\"\n",
    "    list_of_peptides = []\n",
    "\n",
    "    for peptide in lst_of_gff:\n",
    "        # Splitting the gene attribute to extract the first protein name\n",
    "        gene_attribute_parts = peptide[0].split()\n",
    "        first_protein_name = gene_attribute_parts[0]\n",
    "\n",
    "        signal_peptides = {\n",
    "            \"gene\": first_protein_name,\n",
    "            \"start_pos\": int(peptide[3]), #was - 1,\n",
    "            \"end_pos\": int(peptide[4]), #was + 1,\n",
    "            \"signal_peptide_likelyhood\": peptide[5],\n",
    "        }\n",
    "\n",
    "        list_of_peptides.append(signal_peptides)\n",
    "\n",
    "    return list_of_peptides\n",
    "\n",
    "def process_sequences(sequences: List[SeqRecord], fusion: bool, genes_with_signal_peptides: List[str] = None, N_pos: List[int] = None) -> List[SeqRecord]:\n",
    "    \"\"\"\n",
    "    Process sequences based on the fusion flag and signal peptide information.\n",
    "    \n",
    "    Args:\n",
    "        sequences (List[SeqRecord]): List of input sequences.\n",
    "        fusion (bool): Whether to process for fusion protein use.\n",
    "        genes_with_signal_peptides (List[str], optional): List of gene IDs with signal peptides.\n",
    "        N_pos (List[int], optional): List of positions indicating where the signal peptide ends.\n",
    "    \n",
    "    Returns:\n",
    "        List[SeqRecord]: Processed sequences.\n",
    "    \"\"\"\n",
    "    processed_seqs = []\n",
    "\n",
    "    if fusion:\n",
    "        if genes_with_signal_peptides and N_pos:\n",
    "            for i, gene_id in enumerate(genes_with_signal_peptides):\n",
    "                for seq in sequences:\n",
    "                    if gene_id in seq.id:\n",
    "                        processed_seq = seq.seq[(N_pos[i]+1) * 3:-3]\n",
    "                        processed_seqs.append(SeqRecord(processed_seq, id=seq.id, description=seq.description))\n",
    "                        break\n",
    "        \n",
    "        for seq in sequences:\n",
    "            if not genes_with_signal_peptides or not any(gene_id in seq.id for gene_id in genes_with_signal_peptides):\n",
    "                processed_seq = seq.seq[3:-3]\n",
    "                processed_seqs.append(SeqRecord(processed_seq, id=seq.id, description=seq.description))\n",
    "    else:\n",
    "        processed_seqs = sequences\n",
    "\n",
    "    return processed_seqs\n",
    "\n",
    "def truncate_sequences(sequences, genes_with_signal_peptides=None, N_pos=None):\n",
    "    \"\"\"\n",
    "    Truncates sequences based on signal peptide information or removes the first\n",
    "    and last 3 nucleotides for sequences without signal peptides.\n",
    "\n",
    "    Args:\n",
    "        sequences (list): List of SeqRecord objects.\n",
    "        genes_with_signal_peptides (list, optional): List of gene IDs with signal peptides.\n",
    "        N_pos (list, optional): List of positions indicating where the signal peptide ends.\n",
    "\n",
    "    Returns:\n",
    "        list: List of truncated SeqRecord objects.\n",
    "    \"\"\"\n",
    "    truncated_seqs = []\n",
    "\n",
    "    # Ensure sequences is a list of SeqRecord objects\n",
    "    print(f\"Number of sequences received: {len(sequences)}\")\n",
    "    \n",
    "    if not all(isinstance(seq, SeqRecord) for seq in sequences):\n",
    "        raise TypeError(\"All items in 'sequences' should be instances of Bio.SeqRecord.\")\n",
    "    \n",
    "    # Handle sequences with signal peptides\n",
    "    if genes_with_signal_peptides and N_pos:\n",
    "        for i, gene_id in enumerate(genes_with_signal_peptides):\n",
    "            for seq in sequences:\n",
    "                if gene_id in seq.id:\n",
    "                    \n",
    "                    truncated_seq = seq.seq[(N_pos[i]+1) * 3:-3]  # Seq object, not SeqRecord - This cleaving is done remove one additional codon to remove any potential unwanted protease cleavage. Additionally, the stop codon is also removed.\n",
    "                    truncated_seqs.append(SeqRecord(truncated_seq, id=seq.id, description=seq.description))\n",
    "                    break\n",
    "\n",
    "    # Handle sequences that don't have a signal peptide\n",
    "    for seq in sequences:\n",
    "        if not genes_with_signal_peptides or not any(gene_id in seq.id for gene_id in genes_with_signal_peptides):\n",
    "            truncated_seq = seq.seq[3:-3]  # Work on Seq object. This code is to remove start and stop codons if the sequence is to be used for e.g. a fusion protein.\n",
    "            truncated_seqs.append(SeqRecord(truncated_seq, id=seq.id, description=seq.description))\n",
    "\n",
    "    return truncated_seqs\n",
    "\n",
    "def read_fasta(fasta_path):\n",
    "    \"\"\"\n",
    "    Reads a FASTA file and returns a list of SeqRecord objects.\n",
    "    \"\"\"\n",
    "    return list(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "\n",
    "def primer_ta_neb(primer1, primer2, conc=0.4, prodcode=\"phusion-0\"):\n",
    "    \"\"\"\n",
    "    Calls the NEB API to calculate the annealing temperature for a primer pair.\n",
    "    \"\"\"\n",
    "    url = \"https://tmapi.neb.com/tm/batch\"\n",
    "    seqpairs = [[primer1, primer2]]\n",
    "\n",
    "    input_data = {\"seqpairs\": seqpairs, \"conc\": conc, \"prodcode\": prodcode}\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    res = requests.post(url, data=json.dumps(input_data), headers=headers)\n",
    "    r = json.loads(res.content)\n",
    "\n",
    "    if r[\"success\"]:\n",
    "        return r[\"data\"][0][\"ta\"]\n",
    "    else:\n",
    "        print(\"Request failed:\", r[\"error\"][0])\n",
    "        return None\n",
    "\n",
    "def primer_tm_neb1(primer, conc=0.4, prodcode=\"phusion-0\"):\n",
    "    \"\"\"\n",
    "    Calls the NEB API to calculate the melting temperature for a single primer.\n",
    "    \"\"\"\n",
    "    url = \"https://tmapi.neb.com/tm/batch\"\n",
    "    seqpairs = [[primer]]\n",
    "\n",
    "    input_data = {\"seqpairs\": seqpairs, \"conc\": conc, \"prodcode\": prodcode}\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    res = requests.post(url, data=json.dumps(input_data), headers=headers)\n",
    "    r = json.loads(res.content)\n",
    "\n",
    "    if r[\"success\"]:\n",
    "        return r[\"data\"][0][\"tm1\"]\n",
    "    else:\n",
    "        print(\"Request failed:\", r[\"error\"][0])\n",
    "        return None\n",
    "\n",
    "def extract_cds_identifiers(cds_fasta):\n",
    "    \"\"\"\n",
    "    Extracts the protein identifiers from CDS FASTA file headers.\n",
    "\n",
    "    Args:\n",
    "        cds_fasta (str): Path to the CDS FASTA file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with protein_id as the key and the full SeqRecord as the value.\n",
    "    \"\"\"\n",
    "    cds_ids = {}\n",
    "    pattern = r\"\\[protein_id=([A-Za-z0-9_]+\\.\\d+)\\]\"  # Match [protein_id=...]\n",
    "\n",
    "    with open(cds_fasta) as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            match = re.search(pattern, record.description)\n",
    "            if match:\n",
    "                protein_id = match.group(1)\n",
    "                cds_ids[protein_id] = record  # Store the full SeqRecord\n",
    "            else:\n",
    "                # If no protein ID, use the first term from the header\n",
    "                first_term = record.id.split()[0]  # This should capture the core part of the ID\n",
    "                cds_ids[first_term] = record\n",
    "\n",
    "    return cds_ids\n",
    "\n",
    "def generate_amplicons_with_names(truncated_sequences, target_tm=58, limit=10, cds_identifiers=None):\n",
    "    \"\"\"\n",
    "    Generates primers with target melting temperature and names them using protein identifiers.\n",
    "    \n",
    "    Args:\n",
    "        truncated_sequences: List of truncated sequences\n",
    "        target_tm: Target melting temperature\n",
    "        limit: Limit for primer length\n",
    "        cds_identifiers: Dict of CDS identifiers from FASTA headers\n",
    "\n",
    "    Returns:\n",
    "        List of amplicons with appropriate primer names and list of gene names.\n",
    "    \"\"\"\n",
    "    from pydna.design import primer_design\n",
    "    from pydna.dseqrecord import Dseqrecord\n",
    "    from pydna.dseq import Dseq\n",
    "\n",
    "    amplicons = []\n",
    "    gene_names = []\n",
    "\n",
    "    for seq in truncated_sequences:\n",
    "        # Convert Biopython SeqRecord to pydna Dseqrecord\n",
    "        pydna_seq = Dseqrecord(Dseq(str(seq.seq)), linear=True)\n",
    "        \n",
    "        # Find the correct protein ID from cds_identifiers\n",
    "        protein_id = None\n",
    "        for key in cds_identifiers:\n",
    "            if key in seq.id or key in seq.description:\n",
    "                protein_id = key\n",
    "                break\n",
    "        \n",
    "        if not protein_id:\n",
    "            # Fallback to the first part of the sequence id\n",
    "            protein_id = seq.id.split()[0]\n",
    "        \n",
    "        # Generate primers without overhangs\n",
    "        amplicon = primer_design(\n",
    "            pydna_seq,\n",
    "            target_tm=target_tm,\n",
    "            limit=limit,\n",
    "            tm_function=primer_tm_neb1\n",
    "        )\n",
    "\n",
    "        # Assign names using the protein ID\n",
    "        amplicon.forward_primer.id = f\"{protein_id}_fw\"\n",
    "        amplicon.reverse_primer.id = f\"{protein_id}_rv\"\n",
    "        \n",
    "        # Append to amplicons and gene_names\n",
    "        amplicons.append(amplicon)\n",
    "        gene_names.append(protein_id)\n",
    "\n",
    "    return amplicons, gene_names\n",
    "\n",
    "def add_overhangs_and_calculate_tm(amplicons, forward_overhang, reverse_overhang, max_primer_length=60, truncate=False):\n",
    "    \"\"\"\n",
    "    Adds overhangs to primers and calculates the melting temperatures (Tm) for primers. \n",
    "    Optionally truncates overhangs to fit the maximum primer length.\n",
    "    \"\"\"\n",
    "    def truncate_overhang(overhang, primer, max_length):\n",
    "        if len(overhang) + len(primer) > max_length:\n",
    "            overhang = overhang[-(max_length - len(primer)):]  # Only truncate if necessary\n",
    "        return overhang + primer\n",
    "\n",
    "    forward_primer_with_overhang = [\n",
    "        truncate_overhang(forward_overhang, str(f.forward_primer.seq), max_primer_length) if truncate else forward_overhang + str(f.forward_primer.seq)\n",
    "        for f in amplicons\n",
    "    ]\n",
    "    reverse_primer_with_overhang = [\n",
    "        truncate_overhang(reverse_overhang, str(r.reverse_primer.seq), max_primer_length) if truncate else reverse_overhang + str(r.reverse_primer.seq)\n",
    "        for r in amplicons\n",
    "    ]\n",
    "\n",
    "    # Calculate annealing temperatures\n",
    "    aneal_f = [primer_tm_neb1(primer) for primer in forward_primer_with_overhang]\n",
    "    aneal_r = [primer_tm_neb1(primer) for primer in reverse_primer_with_overhang]\n",
    "    ta = [primer_ta_neb(str(f.forward_primer.seq), str(f.reverse_primer.seq)) for f in amplicons]\n",
    "\n",
    "    return forward_primer_with_overhang, reverse_primer_with_overhang, aneal_f, aneal_r, ta\n",
    "\n",
    "def write_to_excel(forward_primer, reverse_primer, sequence_names, aneal_f, aneal_r, ta, output_file):\n",
    "    \"\"\"\n",
    "    Writes the primers to an Excel file with relevant data.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'template': sequence_names,\n",
    "        'f_primer': forward_primer,\n",
    "        'r_primer': reverse_primer,\n",
    "        'f_tm': aneal_f,\n",
    "        'r_tm': aneal_r,\n",
    "        'ta': ta\n",
    "    })\n",
    "\n",
    "    df['len_fw'] = df['f_primer'].apply(lambda x: len(x))\n",
    "    df['len_rv'] = df['r_primer'].apply(lambda x: len(x))\n",
    "\n",
    "    # Generate gene names for forward and reverse primers\n",
    "    gene_names_fw = [name + '_fw' for name in sequence_names]\n",
    "    gene_names_rv = [name + '_rv' for name in sequence_names]\n",
    "\n",
    "    forward_df = pd.DataFrame({\n",
    "        'Name': gene_names_fw,\n",
    "        'Sequence': forward_primer,\n",
    "        'Concentration': '25nm',\n",
    "        'Purification': 'STD'\n",
    "    })\n",
    "\n",
    "    reverse_df = pd.DataFrame({\n",
    "        'Name': gene_names_rv,\n",
    "        'Sequence': reverse_primer,\n",
    "        'Concentration': '25nm',\n",
    "        'Purification': 'STD'\n",
    "    })\n",
    "\n",
    "    idt_primers_result = pd.concat([forward_df, reverse_df], ignore_index=True)\n",
    "    idt_primers_result.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Primers saved to {output_file}\")\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse_fasta_and_extract_identifiers(fasta_path):\n",
    "    \"\"\"\n",
    "    Parses a FASTA file and extracts both the sequences and CDS protein identifiers.\n",
    "    \n",
    "    Args:\n",
    "        fasta_path (str): Path to the FASTA file.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Biopython SeqRecords (sequences).\n",
    "        dict: Dictionary of protein_id to full SeqRecord (cds_identifiers).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    cds_identifiers = {}\n",
    "    pattern = r\"\\[protein_id=([A-Za-z0-9_]+\\.\\d+)\\]\"  # Match [protein_id=...]\n",
    "\n",
    "    with open(fasta_path) as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            sequences.append(record)\n",
    "            match = re.search(pattern, record.description)\n",
    "            if match:\n",
    "                protein_id = match.group(1)\n",
    "                cds_identifiers[protein_id] = record  # Store the full SeqRecord\n",
    "            else:\n",
    "                # If no protein ID, use the first term from the header\n",
    "                first_term = record.id.split()[0]  # This should capture the core part of the ID\n",
    "                cds_identifiers[first_term] = record\n",
    "\n",
    "    return sequences, cds_identifiers\n",
    "\n",
    "def main(gff3_path=None, fasta_path=None, output_file=None, forward_overhang=\"TTTTTTTTTT\", reverse_overhang=\"CCCCCCCC\", \n",
    "         max_primer_length=60, cutoff=False, target_tm=60, limit=14, fusion=False):\n",
    "    \"\"\"\n",
    "    Main function that reads GFF3 and FASTA files, generates primers, and writes the result to an Excel file.\n",
    "    If no GFF3 file is provided, genes will not be truncated, and this will be indicated in the output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse FASTA and extract both sequences and CDS identifiers\n",
    "    sequences, cds_identifiers = parse_fasta_and_extract_identifiers(fasta_path)\n",
    "\n",
    "    # If GFF3 file is provided, read it\n",
    "    if gff3_path:\n",
    "        _, genes_with_signal_peptides, N_pos = read_gff3_and_fasta(gff3_path, fasta_path)\n",
    "    else:\n",
    "        print(\"No GFF3 file provided. Genes will not be truncated.\")\n",
    "        genes_with_signal_peptides = []\n",
    "        N_pos = None\n",
    "\n",
    "    # Process sequences based on fusion flag\n",
    "    processed_sequences = process_sequences(sequences, fusion, genes_with_signal_peptides, N_pos)\n",
    "\n",
    "    # Generate amplicons\n",
    "    amplicons, gene_names = generate_amplicons_with_names(processed_sequences, target_tm=target_tm, limit=limit, cds_identifiers=cds_identifiers)\n",
    "\n",
    "    # Add overhangs and calculate TM, with optional cutoff\n",
    "    forward_primer, reverse_primer, aneal_f, aneal_r, ta = add_overhangs_and_calculate_tm(amplicons, forward_overhang, reverse_overhang, max_primer_length, truncate=cutoff)\n",
    "\n",
    "    # Write results to Excel\n",
    "    write_to_excel(forward_primer, reverse_primer, gene_names, aneal_f, aneal_r, ta, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Generate primers with optional truncations, overhangs and accommodation for fusion protein use.\")\n",
    "\n",
    "    parser.add_argument(\"-f\", \"--fasta\", required=True, help=\"Path to the input FASTA file\")\n",
    "    parser.add_argument(\"-g\", \"--gff3\", help=\"Path to the GFF3 file for truncating the sequences. If omitted, no truncation will be performed.\", default=None)\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"Path to the output Excel file.\", required=True)\n",
    "    parser.add_argument(\"--forward_overhang\", help=\"Forward primer overhang sequence.\", default=\"TTTTTTTTTT\")\n",
    "    parser.add_argument(\"--reverse_overhang\", help=\"Reverse primer overhang sequence.\", default=\"CCCCCCCC\")\n",
    "    parser.add_argument(\"--max_length\", type=int, help=\"Maximum primer length including overhangs.\", default=60)\n",
    "    parser.add_argument(\"--cutoff\", action=\"store_true\", help=\"Cutoff overhangs to fit within the maximum primer length.\")\n",
    "    parser.add_argument(\"-tm\", \"--target_tm\", type=int, help=\"Target melting temperature for primers as per the NEB Calculator for Phusion Polymerases. Default value is 60.\", default=60)\n",
    "    parser.add_argument(\"-lm\", \"--limit\", type=int, help=\"Limit for primer length. Default value is 14.\", default=14)\n",
    "    parser.add_argument(\"--fusion\", action=\"store_true\", help=\"Process sequences for fusion protein use by removing signal peptides and start/stop codons\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(fasta_path=args.fasta,\n",
    "         gff3_path=args.gff3,\n",
    "         output_file=args.output,\n",
    "         forward_overhang=args.forward_overhang,\n",
    "         reverse_overhang=args.reverse_overhang,\n",
    "         max_primer_length=args.max_length,\n",
    "         cutoff=args.cutoff,\n",
    "         target_tm=args.target_tm,\n",
    "         limit=args.limit,\n",
    "         fusion=args.fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
